# Data Storage Configuration
# Parquet-based storage with train/val/test splitting and CV support

storage:
  # Base storage configuration
  base:
    base_path: "data"  # Root directory for all data
    create_dirs: true  # Auto-create directories if missing
    atomic_writes: true  # Use temp file + rename pattern

  # Storage paths
  paths:
    raw: "data/raw"
    processed: "data/processed"
    training: "data/training"
    validation: "data/validation"
    testing: "data/testing"
    checkpoints: "data/checkpoints"
    cache: "data/raw"  # Cache stored with raw data

  # File format configuration
  file_format:
    format: "parquet"  # Options: parquet, csv, hdf5, feather
    compression: "snappy"  # Options: snappy, gzip, brotli, lz4, zstd, none
    engine: "pyarrow"  # Options: pyarrow, fastparquet

    # Parquet-specific settings
    parquet:
      version: "2.6"  # Parquet format version
      use_dictionary: true  # Dictionary encoding for compression
      write_statistics: true  # Write column statistics
      row_group_size: 100000  # Rows per row group
      data_page_size: 1048576  # 1MB page size

    # CSV settings (fallback)
    csv:
      delimiter: ","
      encoding: "utf-8"
      index: true
      header: true

  # File naming conventions
  naming:
    timestamp_format: "%Y%m%d_%H%M%S"
    include_timestamp: true
    include_ticker: false  # Ticker in filename (for raw data only)

    # Templates
    raw_template: "{ticker}_{timestamp}.parquet"
    processed_template: "processed_{timestamp}.parquet"
    split_template: "{split_name}_{timestamp}.parquet"
    cv_fold_template: "fold{fold_id}_{split_type}_{timestamp}.parquet"

  # Data splitting configuration
  data_split:
    # Default split method
    default_method: "simple"  # Options: simple, cv

    # Simple chronological split (70/15/15)
    simple:
      train_ratio: 0.70
      validation_ratio: 0.15
      test_ratio: 0.15  # Auto-calculated (1 - train - val)
      chronological: true  # Preserve temporal ordering
      shuffle: false  # Never shuffle time series data

    # k-fold cross-validation
    cross_validation:
      n_splits: 5  # Number of folds (default k=5)
      test_size: 0.15  # Isolated test set proportion
      gap: 0  # Gap between train/val (in periods)
      expanding_window: true  # Use expanding window strategy

      # Window strategies
      # - expanding: Each fold trains on progressively more data
      # - sliding: Fixed window size slides through data
      window_strategy: "expanding"

      # Test isolation
      strict_isolation: true  # Test never exposed during CV
      isolate_test_first: true  # Remove test set before CV

    # Validation
    validate_splits:
      check_no_overlap: true  # Ensure no data overlap
      check_coverage: true  # Ensure data coverage
      check_temporal_order: true  # Ensure chronological order
      min_split_size: 10  # Minimum observations per split

  # Cache management
  cache:
    enabled: true
    cache_validity_hours: 24
    auto_cleanup: true
    retention_days: 365  # Keep cached data for 1 year
    max_cache_size_gb: 10  # Maximum cache size

    # Cache cleanup strategy
    cleanup:
      strategy: "lru"  # Options: lru (least recently used), fifo, size
      cleanup_interval_hours: 24
      keep_minimum_files: 10

  # Data versioning
  versioning:
    enabled: false  # Disabled by default
    strategy: "timestamp"  # Options: timestamp, hash, sequential
    keep_versions: 5  # Number of versions to keep
    version_dir: "data/versions"

  # Data integrity
  integrity:
    # Checksums
    compute_checksums: false  # MD5 checksums for data files
    verify_on_load: false

    # Backup
    auto_backup: false
    backup_dir: "data/backups"
    backup_interval_hours: 24

  # Metadata tracking
  metadata:
    save_metadata: true
    metadata_dir: "data/metadata"
    metadata_format: "json"

    # Track these attributes
    track:
      file_size: true
      row_count: true
      column_count: true
      creation_time: true
      data_hash: false
      split_ratios: true
      cv_folds: true

  # Performance optimization
  performance:
    # Memory management
    use_memory_mapping: false  # Memory-mapped file I/O
    chunk_size: 10000  # Chunk size for large datasets

    # Compression vs speed trade-off
    compression_level: "default"  # Options: fast, default, maximum

    # Parallel I/O
    parallel_io: false
    n_workers: 4

  # Data loading
  loading:
    # Load optimization
    columns_to_load: null  # null = load all columns
    filters: null  # Parquet predicate pushdown filters

    # Memory optimization
    use_categories: false  # Convert string columns to categories
    downcast_integers: false
    downcast_floats: false

  # Error handling
  error_handling:
    on_save_failure: "raise"  # Options: raise, retry, skip
    on_load_failure: "raise"
    max_retries: 3
    retry_delay_seconds: 1

    # Validation
    validate_before_save: true
    validate_after_load: true

  # Logging
  logging:
    level: "INFO"
    log_operations: true
    log_file: "logs/storage.log"
    log_file_sizes: true
    log_save_times: true

# Storage best practices
best_practices:
  - "Use Parquet format for 10x faster I/O vs CSV"
  - "Enable compression (snappy) for 5-10x storage savings"
  - "Use atomic writes to prevent data corruption"
  - "Preserve temporal ordering for time series data"
  - "Isolate test set strictly (no leakage)"
  - "Use expanding window CV for better coverage"
  - "Enable caching for 20x speedup on repeated access"

# Performance benchmarks (AAPL 1,006 rows)
benchmarks:
  parquet_save: "< 0.1s"
  parquet_load: "< 0.1s"
  csv_save: "0.5s"
  csv_load: "0.8s"
  compression_ratio: "5-10x"
  speedup_vs_csv: "10x"
