# ETL Pipeline Orchestration Configuration
# Combines workflows and data split configuration for modular architecture

pipeline:
  name: "Portfolio Maximizer ETL Pipeline"
  version: "4.0"
  description: "Extract, Transform, Load pipeline with k-fold CV support"

  # Pipeline stages (executed in order)
  stages:
    - name: "data_extraction"
      description: "Extract OHLCV data from Yahoo Finance with caching"
      module: "etl.yfinance_extractor"
      class: "YFinanceExtractor"
      config_file: "config/yfinance_config.yml"
      timeout_seconds: 300
      retry_attempts: 3
      retry_delay: 2
      required: true

    - name: "data_validation"
      description: "Validate data quality (prices, volumes, outliers)"
      module: "etl.data_validator"
      class: "DataValidator"
      config_file: "config/validation_config.yml"
      timeout_seconds: 60
      retry_attempts: 1
      required: true
      on_failure: "warn"  # Options: raise, warn, skip

    - name: "data_preprocessing"
      description: "Handle missing data and normalize features"
      module: "etl.preprocessor"
      class: "Preprocessor"
      config_file: "config/preprocessing_config.yml"
      timeout_seconds: 120
      retry_attempts: 1
      required: true

    - name: "data_storage"
      description: "Split data (simple or CV) and persist to storage"
      module: "etl.data_storage"
      class: "DataStorage"
      config_file: "config/storage_config.yml"
      timeout_seconds: 60
      retry_attempts: 1
      required: true

  # Optional ticker discovery integration
  ticker_discovery:
    enabled: false
    loader: "alpha_vantage"  # Currently supported: alpha_vantage
    cache_dir: "data/tickers"
    universe_path: "data/tickers/ticker_universe.csv"
    fallback_csv: null  # Provide to avoid live network calls
    auto_refresh: false  # Refresh universe automatically before extraction

  # Data splitting configuration
  data_split:
    # Default strategy (config-driven)
    # Change to 'cv' for production-grade k-fold cross-validation
    default_strategy: "cv"  # Options: simple, cv

    # Simple chronological split (70/15/15)
    simple_split:
      enabled: true
      train_ratio: 0.70  # Training set proportion
      validation_ratio: 0.15  # Validation set proportion
      test_ratio: 0.15  # Test set (auto-calculated)
      chronological: true  # Preserve temporal ordering
      shuffle: false  # Never shuffle time series data

    # k-fold cross-validation (recommended for production)
    cross_validation:
      enabled: true  # Can be enabled via --use-cv flag
      n_splits: 5  # Number of CV folds (default k=5)
      test_size: 0.15  # Isolated test set (never in CV)
      gap: 0  # Gap between train/val (periods)
      expanding_window: true  # Use expanding window (vs sliding)

      # Window strategy
      # - expanding: Each fold sees progressively more training data
      # - sliding: Fixed window size slides through data
      window_strategy: "expanding"

      # Validation coverage improvement
      # CV provides 5.5x better temporal coverage (15% â†’ 83%)
      expected_coverage: 0.83  # 83% temporal coverage with k=5

    # Test set isolation (applies to both strategies)
    test_isolation:
      strict_isolation: true  # Test never exposed during training/validation
      no_leakage: true  # Enforce no data leakage
      chronological_order: true  # Test comes after train/val

  # Pipeline execution settings
  execution:
    # Execution mode
    mode: "sequential"  # Options: sequential, parallel (future)

    # Stage dependencies
    enforce_dependencies: true
    fail_fast: false  # Continue on non-critical failures

    # Progress tracking
    progress_bar: true
    log_stages: true
    log_timings: true

    # Checkpointing (save intermediate results)
    checkpoints:
      enabled: true
      checkpoint_dir: "data/checkpoints"
      save_after_each_stage: true
      auto_resume: true  # Resume from last checkpoint on failure

  # Data flow configuration
  data_flow:
    # Storage paths
    storage:
      base_path: "data"
      raw_path: "data/raw"
      processed_path: "data/processed"
      training_path: "data/training"
      validation_path: "data/validation"
      testing_path: "data/testing"

    # File naming convention
    naming:
      timestamp_format: "%Y%m%d_%H%M%S"
      include_ticker: true
      include_timestamp: true

      # Templates
      raw_file: "{ticker}_{timestamp}.parquet"
      processed_file: "processed_{timestamp}.parquet"
      split_file: "{split_name}_{timestamp}.parquet"
      cv_fold_file: "fold{fold_id}_{split_type}_{timestamp}.parquet"

    # File format
    format:
      type: "parquet"  # Options: parquet, csv, hdf5
      compression: "snappy"  # Options: snappy, gzip, brotli, none
      engine: "pyarrow"  # Options: pyarrow, fastparquet

  # Quality assurance
  quality_assurance:
    # Automated tests
    run_tests: true
    test_after_stage: ["data_extraction", "data_preprocessing", "data_storage"]

    # Assertions
    assertions:
      no_empty_datasets: true
      no_duplicate_data: true
      data_types_consistent: true
      temporal_ordering_preserved: true

    # Metrics tracking
    track_metrics: true
    metrics:
      - "rows_processed"
      - "cache_hit_rate"
      - "processing_time"
      - "validation_pass_rate"
      - "test_coverage"

  portfolio_optimizer:
    enabled: false
    price_field: "Close"
    risk_aversion: 1.0
    constraints: null  # Optional dict e.g. {"max_weight": 0.2}

  # Error handling and recovery
  error_handling:
    # Global error strategy
    on_stage_failure: "raise"  # Options: raise, skip, retry
    max_global_retries: 3
    retry_delay_seconds: 5

    # Error logging
    log_errors: true
    error_log_path: "logs/pipeline_errors.log"
    save_error_state: true  # Save pipeline state on error

    # Notifications (future)
    notifications:
      enabled: false
      on_failure: true
      on_success: false
      email: null
      slack_webhook: null

  # Performance optimization
  performance:
    # Caching
    use_cache: true
    cache_validity_hours: 24

    # Memory management
    memory_limit_gb: null  # No limit by default
    chunk_processing: false
    clear_cache_on_complete: false

    # Profiling
    profile_execution: false
    profile_output: "logs/pipeline_profile.log"

  # Logging configuration
  logging:
    level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    log_to_console: true
    log_to_file: true
    log_file: "logs/pipeline.log"
    separate_stage_logs: true
    stage_log_dir: "logs/stages"

# Metadata
metadata:
  created_by: "Portfolio Maximizer v4.0"
  created_date: "2025-10-04"
  last_updated: "2025-10-04"
  purpose: "Unified pipeline and workflow configuration"
  standards: "MIT Statistical Learning Standards"
