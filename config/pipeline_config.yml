# ETL Pipeline Orchestration Configuration
# Combines workflows and data split configuration for modular architecture

pipeline:
  name: "Portfolio Maximizer ETL Pipeline"
  version: "4.0"
  description: "Extract, Transform, Load pipeline with k-fold CV support"

  # Pipeline stages (executed in order)
  stages:
    - name: "data_extraction"
      description: "Extract OHLCV data from configured market data providers (with caching)"
      module: "etl.data_source_manager"
      class: "DataSourceManager"
      config_file: "config/data_sources_config.yml"
      timeout_seconds: 300
      retry_attempts: 3
      retry_delay: 2
      required: true

    - name: "data_validation"
      description: "Validate data quality (prices, volumes, outliers)"
      module: "etl.data_validator"
      class: "DataValidator"
      config_file: "config/validation_config.yml"
      timeout_seconds: 60
      retry_attempts: 1
      required: true
      on_failure: "warn"  # Options: raise, warn, skip

    - name: "data_preprocessing"
      description: "Handle missing data and normalize features"
      module: "etl.preprocessor"
      class: "Preprocessor"
      config_file: "config/preprocessing_config.yml"
      timeout_seconds: 120
      retry_attempts: 1
      required: true

    - name: "data_storage"
      description: "Split data (simple or CV) and persist to storage"
      module: "etl.data_storage"
      class: "DataStorage"
      config_file: "config/storage_config.yml"
      timeout_seconds: 60
      retry_attempts: 1
      required: true

    - name: "time_series_forecasting"
      description: "SARIMAX, GARCH, SAMOSSA, and MSSA-RL forecasting models"
      module: "etl.time_series_forecaster"
      class: "TimeSeriesForecaster"
      config_file: "config/forecasting_config.yml"
      timeout_seconds: 300
      retry_attempts: 1
      required: false
      enabled: true
      depends_on: ["data_storage"]  # Needs processed data

    - name: "time_series_signal_generation"
      description: "Generate trading signals from Time Series forecasts (DEFAULT signal source)"
      module: "models.time_series_signal_generator"
      class: "TimeSeriesSignalGenerator"
      config_file: "config/signal_routing_config.yml"
      timeout_seconds: 120
      retry_attempts: 1
      required: false
      enabled: true
      depends_on: ["time_series_forecasting"]  # Requires forecasts from previous stage

    - name: "signal_router"
      description: "Route signals with Time Series primary, LLM fallback"
      module: "models.signal_router"
      class: "SignalRouter"
      config_file: "config/signal_routing_config.yml"
      timeout_seconds: 60
      retry_attempts: 1
      required: false
      enabled: true
      depends_on: ["time_series_signal_generation"]  # Requires TS signals, optionally LLM signals

  # Optional ticker discovery integration
  ticker_discovery:
    enabled: false
    loader: "alpha_vantage"  # Currently supported: alpha_vantage
    cache_dir: "data/tickers"
    universe_path: "data/tickers/ticker_universe.csv"
    fallback_csv: null  # Provide to avoid live network calls
    auto_refresh: false  # Refresh universe automatically before extraction

  # Data splitting configuration
  data_split:
    # Default strategy (config-driven)
    # Change to 'cv' for production-grade k-fold cross-validation
    default_strategy: "cv"  # Options: simple, cv

    # Simple chronological split (70/15/15)
    simple_split:
      enabled: true
      train_ratio: 0.70  # Training set proportion
      validation_ratio: 0.15  # Validation set proportion
      test_ratio: 0.15  # Test set (auto-calculated)
      chronological: true  # Preserve temporal ordering
      shuffle: false  # Never shuffle time series data

    # k-fold cross-validation (recommended for production)
    cross_validation:
      enabled: true  # Can be enabled via --use-cv flag
      n_splits: 5  # Number of CV folds (default k=5)
      test_size: 0.15  # Isolated test set (never in CV)
      gap: 0  # Gap between train/val (periods)
      expanding_window: true  # Use expanding window (vs sliding)

      # Window strategy
      # - expanding: Each fold sees progressively more training data
      # - sliding: Fixed window size slides through data
      window_strategy: "expanding"

      # Validation coverage improvement
      # CV provides 5.5x better temporal coverage (15% → 83%)
      expected_coverage: 0.83  # 83% temporal coverage with k=5

    # Test set isolation (applies to both strategies)
    test_isolation:
      strict_isolation: true  # Test never exposed during training/validation
      no_leakage: true  # Enforce no data leakage
      chronological_order: true  # Test comes after train/val

  # Pipeline execution settings
  execution:
    # Execution mode
    mode: "sequential"  # Options: sequential, parallel (future)

    # Stage dependencies
    enforce_dependencies: true
    fail_fast: false  # Continue on non-critical failures

    # Progress tracking
    progress_bar: true
    log_stages: true
    log_timings: true

    # Checkpointing (save intermediate results)
    checkpoints:
      enabled: true
      checkpoint_dir: "data/checkpoints"
      save_after_each_stage: true
      auto_resume: true  # Resume from last checkpoint on failure

  # Data flow configuration
  data_flow:
    # Storage paths
    storage:
      base_path: "data"
      raw_path: "data/raw"
      processed_path: "data/processed"
      training_path: "data/training"
      validation_path: "data/validation"
      testing_path: "data/testing"

    # File naming convention
    naming:
      timestamp_format: "%Y%m%d_%H%M%S"
      include_ticker: true
      include_timestamp: true

      # Templates
      raw_file: "{ticker}_{timestamp}.parquet"
      processed_file: "processed_{timestamp}.parquet"
      split_file: "{split_name}_{timestamp}.parquet"
      cv_fold_file: "fold{fold_id}_{split_type}_{timestamp}.parquet"

    # File format
    format:
      type: "parquet"  # Options: parquet, csv, hdf5
      compression: "snappy"  # Options: snappy, gzip, brotli, none
      engine: "pyarrow"  # Options: pyarrow, fastparquet

  # Quality assurance
  quality_assurance:
    # Automated tests
    run_tests: true
    test_after_stage: ["data_extraction", "data_preprocessing", "data_storage"]

  visualization:
    auto_dashboard: true # could reverse to false to disable auto-generation of dashboards
    output_dir: "visualizations"
    lookback_days: 180
    generate_forecast_dashboard: true
    generate_signal_dashboard: true

    # Assertions
    assertions:
      no_empty_datasets: true
      no_duplicate_data: true
      data_types_consistent: true
      temporal_ordering_preserved: true

    # Metrics tracking
    track_metrics: true
    metrics:
      - "rows_processed"
      - "cache_hit_rate"
      - "processing_time"
      - "validation_pass_rate"
      - "test_coverage"

  portfolio_optimizer:
    enabled: false
    price_field: "Close"
    risk_aversion: 1.0
    constraints: null  # Optional dict e.g. {"max_weight": 0.2}

  # Error handling and recovery
  error_handling:
    # Global error strategy
    on_stage_failure: "raise"  # Options: raise, skip, retry
    max_global_retries: 3
    retry_delay_seconds: 5

    # Error logging
    log_errors: true
    error_log_path: "logs/pipeline_errors.log"
    save_error_state: true  # Save pipeline state on error

    # Notifications (future)
    notifications:
      enabled: false
      on_failure: true
      on_success: false
      email: null
      slack_webhook: null

  # Performance optimization
  performance:
    # Caching
    use_cache: true
    cache_validity_hours: 24

    # Memory management
    memory_limit_gb: null  # No limit by default
    chunk_processing: false
    clear_cache_on_complete: false

    # Profiling
    profile_execution: false
    profile_output: "logs/pipeline_profile.log"

  # Logging configuration
  # Time Series Forecasting Configuration (Phase 7.3 - added ensemble settings)
  forecasting:
    enabled: true
    default_forecast_horizon: 30
    minimum_history_required: 90
    minimum_history_strict: 30

    sarimax:
      enabled: false  # Disabled by default; slow grid search
      auto_select_order: true
      max_p: 3
      max_d: 1
      max_q: 2
      seasonal_periods: null
      max_P: 2
      max_D: 1
      max_Q: 2
      trend: "c"
      enforce_stationarity: true
      enforce_invertibility: true

    garch:
      enabled: true
      p: 1
      q: 1
      vol: "GARCH"
      dist: "normal"

    samossa:
      enabled: true
      window_length: 60
      n_components: 8
      use_residual_arima: true
      arima_order: [1, 0, 1]
      seasonal_order: [0, 0, 0, 0]
      min_series_length: 120
      max_forecast_steps: 63
      reconstruction_method: "diagonal_averaging"

    mssa_rl:
      enabled: true
      window_length: 30
      rank: null
      change_point_threshold: 3.5
      q_learning_alpha: 0.3
      q_learning_gamma: 0.85
      q_learning_epsilon: 0.1
      use_gpu: true
      min_series_length: 150
      max_forecast_steps: 30

    combined:
      enabled: true
      use_sarimax_mean: true
      use_garch_volatility: true
      combine_confidence_intervals: true

    ensemble:
      enabled: true
      confidence_scaling: false
      candidate_weights:
        - {garch: 0.85, samossa: 0.10, mssa_rl: 0.05}
        - {garch: 0.70, samossa: 0.20, mssa_rl: 0.10}
        - {garch: 0.60, samossa: 0.25, mssa_rl: 0.15}
        - {samossa: 0.60, mssa_rl: 0.40}
        - {samossa: 0.45, garch: 0.35, mssa_rl: 0.20}
        - {garch: 1.0}
        - {samossa: 1.0}
        - {mssa_rl: 1.0}
      minimum_component_weight: 0.05

    # Phase 7.5: Regime Detection for Adaptive Model Selection
    regime_detection:
      enabled: true  # Phase 7.7: Enabled to test optimized MODERATE_TRENDING weights
      lookback_window: 60  # Days to analyze for regime classification

      # Volatility thresholds (annualized)
      vol_threshold_low: 0.15   # Below 15% annual vol = low volatility regime
      vol_threshold_high: 0.30  # Above 30% annual vol = high volatility regime

      # Trend strength thresholds (0-1 scale, based on R² from linear regression)
      trend_threshold_weak: 0.30    # Below 0.30 = weak/no trend (rangebound)
      trend_threshold_strong: 0.60  # Above 0.60 = strong directional trend

      # Phase 7.8: Regime-specific ensemble candidate weights (optimized via rolling CV)
      # When present, these replace pipeline.forecasting.ensemble.candidate_weights for the detected regime.
      # Generated via: python scripts/optimize_ensemble_weights.py --source rolling_cv
      # Phase 7.8 Results (2026-01-27): 3 regimes optimized with SAMOSSA-dominant weights
      regime_candidate_weights:
        CRISIS:
          # Optimized for 50%+ volatility, weak trend, 5 CV folds, 25 samples
          # RMSE: 17.15 -> 6.74 (+60.69% improvement)
          - {garch: 0.23, samossa: 0.72, mssa_rl: 0.05}
        MODERATE_MIXED:
          # Optimized for 26-30% volatility, mixed trend (R²=0.00-0.30), 4 CV folds, 20 samples
          # RMSE: 17.63 -> 16.52 (+6.30% improvement)
          - {garch: 0.05, samossa: 0.73, mssa_rl: 0.22}
        MODERATE_TRENDING:
          # Optimized for 18-23% volatility, strong trend (R²>0.73), 10 CV folds, 50 samples
          # RMSE: 20.86 -> 7.29 (+65.07% improvement)
          - {garch: 0.05, samossa: 0.90, mssa_rl: 0.05}

      # Regime-specific model preferences
      regime_model_preferences:
        LIQUID_RANGEBOUND:
          preferred_models: ['garch', 'samossa', 'mssa_rl']
          description: "Stable low-volatility markets, GARCH volatility forecasting optimal"

        MODERATE_RANGEBOUND:
          preferred_models: ['garch', 'samossa', 'mssa_rl']
          description: "Low volatility with some directional bias"

        MODERATE_TRENDING:
          preferred_models: ['samossa', 'garch', 'mssa_rl']
          description: "Clear trend with moderate volatility"

        HIGH_VOL_TRENDING:
          preferred_models: ['samossa', 'mssa_rl', 'garch']
          description: "Volatile directional moves, complex patterns"

        CRISIS:
          preferred_models: ['garch', 'samossa']
          description: "Crisis mode, focus on volatility and simple models"

        MODERATE_MIXED:
          preferred_models: ['garch', 'samossa', 'mssa_rl']
          description: "No clear regime, use balanced ensemble"

  logging:
    level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    log_to_console: true
    log_to_file: true
    log_file: "logs/pipeline.log"
    separate_stage_logs: true
    stage_log_dir: "logs/stages"

# Metadata
metadata:
  created_by: "Portfolio Maximizer v4.0"
  created_date: "2025-10-04"
  last_updated: "2025-10-04"
  purpose: "Unified pipeline and workflow configuration"
  standards: "MIT Statistical Learning Standards"
