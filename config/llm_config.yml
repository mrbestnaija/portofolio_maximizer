# LLM Configuration - Local GPU AI Integration
# Phase 5.2: Production LLM for Portfolio Analysis
# Cost: $0/month (local GPU via Ollama)

llm:
  # Ollama server configuration
  server:
    host: "http://localhost:11434"
    timeout_seconds: 30  # Max wait for LLM response (fast-fail)
    health_check_on_init: true  # Validate on startup (fail-fast)

  # Model selection - 3-model local strategy (2026-02-16)
  # deepseek-r1 = reasoning/CoT, qwen3 = tool-calling orchestrator
  models:
    # REASONING MODEL: DeepSeek R1 8B for fast chain-of-thought
    primary:
      name: "deepseek-r1:8b"
      role: "reasoning"
      use_case: "Fast reasoning for quantitative analysis, regime detection, signal validation"
      expected_speed: "25-35 tokens/sec"
      memory_requirement: "5.5GB VRAM"
      quality: "excellent"
      capabilities: ["chain-of-thought", "math", "code-generation"]
      priority: 1

    # HEAVY REASONING: DeepSeek R1 32B for deep analysis
    fallback_1:
      name: "deepseek-r1:32b"
      role: "heavy_reasoning"
      use_case: "Deep multi-step financial analysis, adversarial audits, complex forecasting"
      expected_speed: "10-15 tokens/sec"
      memory_requirement: "20GB VRAM"
      quality: "excellent"
      capabilities: ["chain-of-thought", "math", "code-generation", "long-context"]
      priority: 2

    # TOOLING MODEL: Qwen3 8B for function-calling and orchestration
    fallback_2:
      name: "qwen3:8b"
      role: "tooling"
      use_case: "Tool/function calling, structured output, API orchestration, social media automation"
      expected_speed: "30-40 tokens/sec"
      memory_requirement: "5.5GB VRAM"
      quality: "excellent"
      capabilities: ["tool-calling", "function-calling", "structured-output", "thinking-mode"]
      priority: 3

  # Active model (default for general inference)
  active_model: "deepseek-r1:8b"

  # Multi-model routing: qwen3 orchestrates, deepseek reasons
  routing:
    # Use qwen3:8b when the task needs tool-calling or structured output
    tool_calling_model: "qwen3:8b"
    # Use deepseek-r1:8b for fast reasoning tasks
    reasoning_model: "deepseek-r1:8b"
    # Use deepseek-r1:32b for complex multi-step analysis
    heavy_reasoning_model: "deepseek-r1:32b"
    # Task-to-model mapping
    task_routing:
      market_analysis: "deepseek-r1:8b"
      signal_generation: "deepseek-r1:8b"
      risk_assessment: "deepseek-r1:8b"
      portfolio_optimization: "deepseek-r1:32b"
      adversarial_audit: "deepseek-r1:32b"
      api_orchestration: "qwen3:8b"
      notification_formatting: "qwen3:8b"
      data_extraction: "qwen3:8b"
      social_media_automation: "qwen3:8b"

  # Generation parameters
  generation:
    temperature: 0.1  # Low for consistency (0.0-1.0)
    max_tokens: 768  # Limit output length for latency control
    max_prompt_chars: 2800  # Trim prompts to reduce latency
    top_p: 0.9  # Nucleus sampling threshold
    top_k: 40  # Top-k sampling
    repeat_penalty: 1.1  # Penalize repetition

  # Market analyzer settings
  market_analyzer:
    enabled: true
    system_prompt: "You are a quantitative financial analyst. Provide concise, data-driven market analysis. Output valid JSON only."
    temperature: 0.1
    required_fields:
      - trend  # bullish, bearish, neutral
      - strength  # 1-10 scale
      - regime  # trending, ranging, volatile, stable
      - summary  # Brief 2-sentence analysis

  # Signal generator settings
  signal_generator:
    enabled: true
    system_prompt: "You are a quantitative trading strategist. Generate trading signals based on data analysis. Be conservative. Output valid JSON only."
    temperature: 0.05  # Very low for signal consistency
    required_fields:
      - action  # BUY, SELL, HOLD
      - confidence  # 0.0-1.0
      - reasoning  # Justification
      - risk_level  # low, medium, high

    # Signal validation rules (per AGENT_INSTRUCTION.md)
    validation:
      version: v2  # Activate advanced SignalValidator integration
      min_confidence_for_action: 0.75  # Require 75% confidence for BUY/SELL
      require_reasoning: true
      conservative_bias: true  # Default to HOLD unless strong signal
      max_volatility_percentile: 0.95  # Reject signals in top 5% volatility regimes
      max_position_size: 0.02  # Cap at 2% of portfolio
      transaction_cost: 0.001  # 10 bps transaction cost assumption
      portfolio_notional: 10000  # Default portfolio size for sizing checks

  # Risk assessor settings
  risk_assessor:
    enabled: true
    system_prompt: "You are a quantitative risk analyst. Assess portfolio risk based on statistical metrics. Output valid JSON only."
    temperature: 0.1
    required_fields:
      - risk_level  # low, medium, high, extreme
      - risk_score  # 0-100
      - concerns  # List of risk factors
      - recommendation  # Position sizing advice

  # Performance tracking
  performance:
    log_latency: true
    log_token_count: true
    log_model_used: true
    enable_cache: true  # Keep in-memory response cache active
    track_cache_usage: true  # Log cache utilisation metrics
    cache_max_size: 64
    cache_ttl_seconds: 300  # Expire cached generations after 5 minutes
    default_use_case: fast  # Optimizer biases towards latency
    latency_failover_threshold: 6.0  # Seconds before switching to faster model
    token_rate_failover_threshold: 12.0  # Tokens/sec floor before attempting fallback

  # Error handling
  error_handling:
    on_connection_error: "fail"  # Pipeline stops if Ollama unavailable
    on_parse_error: "default"  # Use safe default values
    on_timeout: "retry_once"  # Retry once then fail
    log_errors: true
    error_log_path: "logs/llm_errors.log"

  # Cost tracking (local GPU = $0/month)
  cost:
    api_cost_per_request: 0.0  # Free (local GPU)
    electricity_cost_estimate: 0.0  # Negligible
    total_monthly_budget: 0.0  # No API costs

  # Data privacy (critical requirement)
  privacy:
    data_leaves_local_machine: false  # All processing local
    no_external_api_calls: true
    sensitive_data_handling: "local_only"
    compliance: "full_data_privacy"

# Hardware requirements (per TO_DO_LLM_local.mdc)
hardware:
  gpu:
    model: "RTX 4060 Ti 16GB"  # Or equivalent
    vram_required: "16GB minimum"
    vram_available: true

  ram:
    minimum: "16GB"
    recommended: "32GB"
    for_33b_model: "20GB"

  storage:
    model_size_33b: "~19GB"
    model_size_13b: "~7GB"
    model_size_14b: "~8GB"
    total_required: "~35GB for all models"

# Installation instructions
installation:
  ollama:
    install_command: "curl -s https://raw.githubusercontent.com/ollama/ollama/main/install.sh | sh"
    start_command: "ollama serve"

  models:
    pull_reasoning: "ollama pull deepseek-r1:8b"
    pull_heavy: "ollama pull deepseek-r1:32b"
    pull_tooling: "ollama pull qwen3:8b"

# Validation rules (per AGENT_INSTRUCTION.md)
validation:
  # CRITICAL: LLM signals are advisory only
  signals_require_backtesting: true
  min_backtest_days: 30
  min_annual_return: 0.10  # 10% minimum
  must_beat_buy_and_hold: true

  # NO TRADING until proven profitable
  trading_enabled: false  # Set true only after validation
  paper_trading_first: true
  live_trading_minimum_capital: 10000  # $10K minimum

# Metadata
metadata:
  created_by: "Portfolio Maximizer v5.2"
  created_date: "2025-10-12"
  phase: "5.2 - Local LLM Integration"
  status: "Production Ready"
  cost_per_month: "$0 (local GPU)"
  data_privacy: "100% local processing"

