---
alwaysApply: false
---

```mdc
# Local GPU AI Integration Guide

## 🚀 Quick Setup

### 1. Install Ollama
```bash
curl -s https://raw.githubusercontent.com/ollama/ollama/main/install.sh | sh
```

### 2. Download Production Models
```bash
ollama pull deepseek-coder:33b-instruct-q4_K_M
ollama pull codellama:13b-instruct-q4_K_M  
ollama pull qwen:14b-chat-q4_K_M
```

### 3. Start Development
```bash
cd /mnt/c/Users/Bestman/personal_projects/portfolio_maximizer_v45
python ai_development/console.py
```

## 🎯 Hardware Optimization

### RTX 4060 Ti 16GB Configuration
```python
MODEL_STRATEGY = {
    'primary_coding': 'deepseek-coder:33b-instruct-q4_K_M',  # 20GB RAM
    'fast_coding': 'codellama:13b-instruct-q4_K_M',         # 8GB RAM
    'reasoning': 'qwen:14b-chat-q4_K_M',                    # 9GB RAM
    'lightning': 'deepseek-coder:6.7b-instruct-q4_K_M'      # 4GB RAM
}
```

### Expected Performance
| Model | Speed | Quality | Use Case |
|-------|--------|---------|----------|
| DeepSeek 33B | 15-20 t/s | 🏆 Excellent | Primary development |
| CodeLlama 13B | 25-35 t/s | 🥈 Very Good | Fast iterations |
| Qwen 14B | 20-25 t/s | 🥈 Great | Financial reasoning |

## 📁 Project Structure

```
portfolio_maximizer_v45/
├── ai_development/
│   ├── agent_client.py              # Main AI client
│   ├── markdown_loader.py           # Dynamic guideline loader
│   ├── workflow_orchestrator.py     # Implementation orchestrator
│   └── guidelines/                  # AI-readable standards
│       ├── coding_standards.md
│       └── architecture_patterns.md
├── Documentation/                   # Existing docs
│   ├── AGENT_INSTRUCTION.md
│   ├── AGENT_DEV_CHECKLIST.md
│   └── implementation_checkpoint.md
└── [existing modules...]
```

## 🛠️ Core AI Client

```python
# ai_development/agent_client.py
class ProductionAIClient:
    """Production-ready AI client for local GPU development"""
    
    def generate_component(self, component_name: str, description: str, 
                          target_file: str) -> Dict[str, Any]:
        """Generate project components using markdown-driven guidelines"""
        
    def chat(self, message: str) -> str:
        """Interactive chat for project discussions"""
        
    def generate_test_file(self, code_file_path: str) -> Dict[str, Any]:
        """Generate corresponding tests for generated code"""
```

## 🎯 Usage Examples

### 1. Component Generation
```python
from ai_development.agent_client import ai_client

result = ai_client.generate_component(
    component_name="ML Forecasting Pipeline",
    description="Multi-horizon forecasting with ensemble models",
    target_file="ml/forecasting/quantitative_forecaster.py"
)
```

### 2. Interactive Development
```python
# Start interactive console
python ai_development/console.py

# Available commands:
# generate <task>    - Generate code for a task
# chat <message>     - Chat with AI about project
# implement          - Run automated implementation
```

### 3. Automated Workflow
```python
from ai_development.workflow_orchestrator import DynamicWorkflowOrchestrator

orchestrator = DynamicWorkflowOrchestrator()
results = orchestrator.execute_suggested_implementation()
```

## 🔧 Key Features

### Markdown-Driven Development
- Reads guidelines from `Documentation/` and `ai_development/guidelines/`
- Dynamic context loading from project status
- No hardcoded phases - completely adaptive

### Production Code Standards
- Type hints and comprehensive docstrings
- Error handling and logging
- Unit test generation
- Project pattern compliance

### GPU-Optimized Performance
- 15-25 tokens/second on RTX 4060 Ti
- Automatic model fallback
- Memory-efficient operation

## 📊 Success Metrics

### Code Quality Targets
```python
QUALITY_TARGETS = {
    'syntax_valid': '100% generated code compiles',
    'type_hints': '>90% functions with type hints', 
    'docstrings': '100% public methods documented',
    'test_coverage': 'Generated tests for all components'
}
```

### Performance Benchmarks
- **Code Generation**: 100-200 lines/minute
- **Response Time**: 2-10 seconds for typical tasks
- **Model Accuracy**: >85% syntax-valid code generation

## 🛡️ Safety Protocols

### Development Guards
```
NEVER:
- Break existing production functionality
- Modify core ETL without testing
- Bypass risk management controls

ALWAYS:
- Maintain backward compatibility  
- Validate against project standards
- Generate corresponding tests
```

### Model Risk Controls
```python
MODEL_SAFETY = {
    'max_file_size': 500,        # lines
    'validation_required': True,  # syntax check
    'test_generation': True,      # auto-generate tests
    'backup_before_write': True   # safety backup
}
```

## 🔄 Integration Points

### With Existing ETL Pipeline
```python
# Reuses existing patterns from:
# - etl/base_extractor.py
# - etl/data_source_manager.py
# - config/pipeline_config.yml
```

### ML Phase 6 Integration
```python
# Generates components for:
# - ml/forecasting/quantitative_forecaster.py
# - trading/ml_strategy_engine.py
# - strategies/ml_barbell_optimizer.py
```

## 🚀 Implementation Priority

### Phase 1: Core Setup (Day 1)
1. Install Ollama and download models
2. Test basic code generation
3. Validate GPU acceleration

### Phase 2: Project Integration (Day 2)  
1. Implement markdown-driven guidelines
2. Generate first components (Alpha Vantage/Finnhub)
3. Establish development workflow

### Phase 3: Production Usage (Day 3+)
1. Automated component generation
2. Test coverage validation
3. Performance optimization

## 💰 Cost Comparison

| Platform | Cost | Privacy | Speed | Usage |
|----------|------|---------|--------|-------|
| **Local AI** | $0 after setup | ✅ Full | Very Fast | Unlimited |
| Claude Team | $30-60/month | ❌ Cloud | Fast | Quota limited |
| OpenAI GPT-4 | $10-50/day | ❌ Cloud | Fast | Token limited |

## 🎯 Immediate Actions

### 1. Start Ollama Service
```bash
ollama serve &
```

### 2. Test Installation
```bash
ollama list
curl http://localhost:11434/api/tags
```

### 3. Generate First Component
```bash
python -c "
from ai_development.agent_client import quick_generate
quick_generate('Create AlphaVantageExtractor implementation', 'etl/alpha_vantage_extractor.py')
"
```

## 📈 Performance Monitoring

### Real-time Metrics
```python
# Track AI performance
performance_tracker.track_request(
    task="ML pipeline generation",
    tokens_generated=1500,
    success=True
)
```

### Quality Assurance
- Syntax validation on all generated code
- Automatic test generation
- Project standards compliance checking
- Integration testing with existing codebase

## 🔧 Troubleshooting

### Common Issues
1. **GPU Memory**: Use smaller models (13B) if 33B doesn't fit
2. **Slow Responses**: Check `ollama ps` for model loading
3. **Syntax Errors**: Regenerate with more specific prompts

### Performance Optimization
```bash
# Monitor GPU usage
nvidia-smi

# Check model performance
python ai_development/scripts/benchmark_models.py
```

---

**Last Updated**: 2025-10-07  
**Hardware**: RTX 4060 Ti 16GB  
**Models**: DeepSeek 33B, CodeLlama 13B, Qwen 14B  
**Status**: Production Ready
```

This condensed .mdc file provides:

## Key Benefits:

### 🚀 **Quick Start**
- 3-step installation process
- Immediate usability

### 🎯 **Hardware Optimized** 
- Specific configuration for RTX 4060 Ti
- Performance expectations

### 📁 **Structured Integration**
- Clear project structure
- Existing documentation reuse

### 🛠️ **Production Ready**
- Code quality standards
- Safety protocols
- Error handling

### 💰 **Cost Effective**
- $0 ongoing costs vs $360-720/year for cloud AI
- Unlimited usage

The guide maintains all critical information while being concise and actionable for immediate implementation.